"""
Module de scraping AliExpress avec recherche par image - VERSION PROFESSIONNELLE
Inspir√© de code production avec fingerprinting, sessions et routing
"""
import asyncio
import os
import re
import random as rnd
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional
from urllib.parse import urljoin, urlparse

from crawlee import (
    ConcurrencySettings,
    Request,
    SkippedReason,
    service_locator,
)
from crawlee.sessions import SessionPool
from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext
from crawlee.storages import Dataset, RequestQueue
from crawlee.fingerprint_suite import (
    DefaultFingerprintGenerator,
    HeaderGeneratorOptions,
    ScreenOptions,
)

from src.models.data_models import ImageMetadata, ProductData


# Configuration
HEADLESS = True
TEMPO_DELAY = 2.0  # D√©lai al√©atoire pour simuler comportement humain


def url_noparams(url: str) -> str:
    """Retire les param√®tres d'une URL"""
    return url.split('?')[0] if '?' in url else url


class AliExpressImageSearchScraper:
    """
    Scraper AliExpress professionnel avec:
    - Fingerprint generation pour √©viter la d√©tection
    - Session pool pour g√©rer les sessions
    - Dataset API pour stockage structur√©
    - Router avec labels pour s√©parer ITEM et ITEM_IMG
    - Upload d'image natif AliExpress
    """

    def __init__(self, output_dir: str = "output"):
        self.output_dir = Path(output_dir)
        self.images_dir = self.output_dir / "images"
        self.images_dir.mkdir(parents=True, exist_ok=True)

        self.attempt_id = f"aliexpress_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.image_counter = 0
        self.target_results = 0

    async def search_by_image(
        self,
        image_path: str,
        category: str = "",
        max_results: int = 50,
        headless: bool = True
    ) -> Tuple[List[ImageMetadata], List[ProductData]]:
        """
        Recherche sur AliExpress par upload d'image

        Args:
            image_path: Chemin vers l'image √† uploader
            category: Cat√©gorie pour filtrer (optionnel)
            max_results: Nombre max de r√©sultats
            headless: Mode headless du navigateur
        """
        self.target_results = max_results
        self.image_counter = 0

        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image not found: {image_path}")

        # Initialiser les datasets
        item_dataset = await Dataset.open(name=f"{self.attempt_id}-items")
        img_dataset = await Dataset.open(name=f"{self.attempt_id}-images")

        # Fingerprint generator - g√©n√®re des empreintes de navigateur r√©alistes
        fingerprint_generator = DefaultFingerprintGenerator(
            header_options=HeaderGeneratorOptions(
                browsers=["chrome", "firefox", "edge"],
                operating_systems=["windows", "linux"],
                devices=["desktop"],
                locales=["en-US", "fr-FR"],
            ),
            screen_options=ScreenOptions(min_width=1024, max_width=1920),
        )

        # Configuration Crawlee personnalis√©e
        custom_config = service_locator.get_configuration()
        custom_config.available_memory_ratio = 0.5
        custom_config.purge_on_start = True

        # Request queue nomm√©e
        request_queue = await RequestQueue.open(name=self.attempt_id)

        # Crawler avec configuration avanc√©e
        crawler = PlaywrightCrawler(
            request_manager=request_queue,
            max_requests_per_crawl=max_results * 2,  # Items + images
            configuration=custom_config,
            headless=headless,
            browser_type="chromium",
            keep_alive=False,
            # Session pool pour g√©rer les sessions
            use_session_pool=True,
            session_pool=SessionPool(
                max_pool_size=10,
                create_session_settings={'max_usage_count': 3}
            ),
            # Fingerprint pour √©viter d√©tection
            fingerprint_generator=fingerprint_generator,
            # Concurrency settings pour performance optimale
            concurrency_settings=ConcurrencySettings(
                min_concurrency=2,
                desired_concurrency=4,
                max_concurrency=6,
                max_tasks_per_minute=20
            )
        )

        # ========================
        # HANDLER: Default (Page de recherche)
        # ========================
        @crawler.router.default_handler
        async def search_handler(context: PlaywrightCrawlingContext) -> None:
            """
            Upload l'image et attend les r√©sultats de recherche AliExpress
            """
            context.log.info(f"üîç Processing {context.request.url}")

            # Tempo delay pour simuler comportement humain
            await asyncio.sleep(1 + (rnd.random() * TEMPO_DELAY))

            page = context.page

            # Fermer popup publicitaire si pr√©sent
            try:
                await page.locator("img[class^=pop-close-btn]").click(timeout=2000)
                context.log.info("‚úÖ Popup ferm√©e")
            except:
                context.log.info("‚ÑπÔ∏è Pas de popup")

            # Cliquer sur "Search by image"
            context.log.info("üì∏ Clic sur recherche par image...")
            try:
                # Plusieurs s√©lecteurs possibles
                search_by_img_selectors = [
                    'img[alt*="search by image"]',
                    'img[alt*="image search"]',
                    'div[class*="search-by-image"]',
                    'button[class*="image-search"]',
                ]

                clicked = False
                for selector in search_by_img_selectors:
                    try:
                        await page.locator(selector).first.click(timeout=3000)
                        clicked = True
                        context.log.info(f"‚úÖ Cliqu√© avec s√©lecteur: {selector}")
                        break
                    except:
                        continue

                if not clicked:
                    # Fallback: essayer avec texte
                    await page.get_by_alt_text("recherche par image").click(timeout=5000)

            except Exception as e:
                context.log.error(f"‚ùå Impossible de trouver le bouton de recherche par image: {e}")
                return

            # Upload de l'image
            context.log.info(f"üì§ Upload de l'image: {image_path}")
            try:
                await page.locator("input[type=file]").set_input_files(image_path)
                context.log.info("‚úÖ Image upload√©e avec succ√®s")
            except Exception as e:
                context.log.error(f"‚ùå Erreur upload image: {e}")
                return

            # Attendre la redirection vers les r√©sultats
            context.log.info("‚è≥ Attente des r√©sultats...")
            try:
                await page.wait_for_url(
                    "**/w/wholesale-*.html*",
                    timeout=30000
                )
                context.log.info(f"‚úÖ R√©sultats charg√©s: {page.url}")
            except:
                context.log.warning("‚ö†Ô∏è URL pattern non d√©tect√©, on continue...")

            await asyncio.sleep(1 + (rnd.random() * TEMPO_DELAY))

            # Scroll infini pour charger les produits lazy-loaded
            context.log.info("üìú Infinite scroll...")
            await context.infinite_scroll()

            # D√©terminer le nombre de pages
            context.log.info("üìÑ D√©tection du nombre de pages...")
            max_pages = 1
            try:
                pagination_texts = await page.locator(
                    "div[class*=pagination]"
                ).all_text_contents()

                if pagination_texts:
                    for text in pagination_texts:
                        matches = re.findall(r'\d+', text)
                        if matches:
                            max_pages = max([int(m) for m in matches])
                            break

                context.log.info(f"üìÑ Nombre de pages d√©tect√©: {max_pages}")
            except Exception as e:
                context.log.warning(f"‚ö†Ô∏è Pagination non d√©tect√©e: {e}")

            # Limiter √† 10 pages max
            max_pages = min(max_pages, 10)

            # Extraire les produits de la page 1
            await self._extract_search_results(context, item_dataset, request_queue)

            # Ajouter les pages suivantes
            current_url = page.url
            base_url = url_noparams(current_url)

            for page_num in range(2, max_pages + 1):
                if '?' in current_url:
                    next_url = f"{base_url}?page={page_num}"
                else:
                    next_url = f"{base_url}&page={page_num}"

                context.log.info(f"üìë Ajout page {page_num}: {next_url}")
                await context.add_requests([
                    Request.from_url(next_url, label="SEARCH_PAGE")
                ])

        # ========================
        # HANDLER: Pages de recherche suivantes
        # ========================
        @crawler.router.handler("SEARCH_PAGE")
        async def search_page_handler(context: PlaywrightCrawlingContext) -> None:
            """Traite les pages de r√©sultats suivantes"""
            context.log.info(f"üìÑ Page de recherche: {context.request.url}")

            await asyncio.sleep(1 + (rnd.random() * TEMPO_DELAY))
            await context.infinite_scroll()

            await self._extract_search_results(context, item_dataset, request_queue)

        # ========================
        # HANDLER: Page produit (ITEM)
        # ========================
        @crawler.router.handler("ITEM")
        async def item_handler(context: PlaywrightCrawlingContext) -> None:
            """Traite une page produit individuelle"""
            if context.response.status in [404, 403]:
                context.log.warning(f"‚ö†Ô∏è Page inaccessible: {context.response.status}")
                await context.page.close()
                return

            context.log.info(f"üõçÔ∏è Traitement produit: {context.request.url}")
            page = context.page
            item_url = url_noparams(context.request.url)

            await asyncio.sleep(1 + (rnd.random() * TEMPO_DELAY))

            # Extraire les donn√©es
            try:
                # Titre
                title = await page.title()
                context.log.info(f"   üìù Titre: {title[:50]}...")

                # Prix
                price = "N/A"
                try:
                    price_elem = await page.locator("span[class^=price], div[class*=price]").first.text_content(timeout=3000)
                    if price_elem:
                        price = price_elem.strip()
                except:
                    pass

                # Images produit
                context.log.info("   üñºÔ∏è Extraction des images...")
                product_imgs = await page.locator(
                    "div[class*=slider] img, div[class*=image-view] img"
                ).all()

                img_links = []
                for pimg in product_imgs[:5]:  # Max 5 images
                    try:
                        src = await pimg.get_attribute("src")
                        if src and 'alicdn' in src:
                            img_links.append(src)
                    except:
                        continue

                context.log.info(f"   ‚úÖ {len(img_links)} images trouv√©es")

                # Ajouter les requ√™tes d'images (PRIORIT√â)
                for img_url in img_links:
                    await request_queue.add_requests([
                        Request.from_url(
                            url=img_url,
                            label="ITEM_IMG",
                            user_data={"product_url": item_url}
                        )
                    ], forefront=True)  # Priorit√© aux images

                # Sauvegarder les donn√©es du produit
                item_data = {
                    "item_url": item_url,
                    "src_image": image_path,
                    "title": title,
                    "description": title,  # Description = titre pour l'instant
                    "collection_date": datetime.now().isoformat(),
                    "price": price,
                    "screenshot_path": "",
                    "product_image_paths": img_links,
                }

                await item_dataset.push_data(item_data)
                context.log.info(f"   ‚úÖ Produit sauvegard√©")

            except Exception as e:
                context.log.error(f"   ‚ùå Erreur extraction produit: {e}")

            await context.page.close()

        # ========================
        # HANDLER: Image produit (ITEM_IMG)
        # ========================
        @crawler.router.handler("ITEM_IMG")
        async def item_img_handler(context: PlaywrightCrawlingContext) -> None:
            """T√©l√©charge une image de produit"""
            img_url = context.response.url
            product_url = context.request.user_data.get("product_url", "")

            try:
                # T√©l√©charger l'image
                self.image_counter += 1
                ext = '.jpg'

                parsed = urlparse(img_url)
                file_ext = os.path.splitext(parsed.path)[1]
                if file_ext in ['.jpg', '.jpeg', '.png', '.webp']:
                    ext = file_ext

                filename = f"image_{self.image_counter:04d}{ext}"
                filepath = self.images_dir / filename

                # Download
                response = await context.page.request.get(img_url, timeout=10000)
                if response.status == 200:
                    with open(filepath, 'wb') as f:
                        f.write(await response.body())

                    context.log.info(f"   üì• Image t√©l√©charg√©e: {filename}")

                    # Sauvegarder les m√©tadonn√©es
                    img_metadata = {
                        "src": img_url,
                        "link": product_url,
                    }
                    await img_dataset.push_data(img_metadata)

            except Exception as e:
                context.log.error(f"   ‚ùå Erreur t√©l√©chargement image: {e}")

            await context.page.close()

        # ========================
        # HANDLER: Requ√™tes bloqu√©es par robots.txt
        # ========================
        @crawler.on_skipped_request
        async def skipped_request_handler(url: str, reason: SkippedReason) -> None:
            if reason == "robots_txt":
                crawler.log.info(f"‚ö†Ô∏è Bloqu√© par robots.txt: {url}")

        # ========================
        # D√©marrer le crawl
        # ========================
        start_url = "https://www.aliexpress.com"
        crawler.log.info(f"üöÄ D√©marrage du crawl: {start_url}")
        crawler.log.info(f"   üì∏ Image: {image_path}")
        crawler.log.info(f"   üéØ Objectif: {max_results} produits")

        await crawler.run([start_url], purge_request_queue=True)

        # ========================
        # Exporter les r√©sultats
        # ========================
        crawler.log.info("üíæ Export des donn√©es...")

        items_json = self.output_dir / "product_data.json"
        images_json = self.output_dir / "image_metadata.json"

        await crawler.export_data(
            path=str(items_json),
            dataset_name=f"{self.attempt_id}-items"
        )
        await crawler.export_data(
            path=str(images_json),
            dataset_name=f"{self.attempt_id}-images"
        )

        crawler.log.info(f"‚úÖ Items export√©s: {items_json}")
        crawler.log.info(f"‚úÖ Images export√©es: {images_json}")

        # Charger et retourner les donn√©es
        image_metadata_list = []
        product_data_list = []

        # Lire les fichiers JSON export√©s
        import json

        if items_json.exists():
            with open(items_json, 'r', encoding='utf-8') as f:
                items_data = json.load(f)
                for item in items_data:
                    product_data_list.append(ProductData(
                        item_url=item['item_url'],
                        collection_date=datetime.fromisoformat(item['collection_date']),
                        src_image=item['src_image'],
                        title=item['title'],
                        description=item['description'],
                        price=item['price'],
                        screenshot_path=item['screenshot_path'],
                        product_image_paths=item['product_image_paths']
                    ))

        if images_json.exists():
            with open(images_json, 'r', encoding='utf-8') as f:
                images_data = json.load(f)
                for img in images_data:
                    image_metadata_list.append(ImageMetadata(
                        src=img['src'],
                        link=img['link']
                    ))

        return image_metadata_list, product_data_list

    async def _extract_search_results(
        self,
        context: PlaywrightCrawlingContext,
        item_dataset: Dataset,
        request_queue: RequestQueue
    ) -> None:
        """
        Extrait les liens produits depuis une page de r√©sultats
        """
        page = context.page
        context.log.info("üîç Extraction des r√©sultats de recherche...")

        try:
            # Attendre que les produits se chargent
            await page.wait_for_timeout(2000)

            # Trouver tous les liens produits
            # AliExpress utilise g√©n√©ralement .search-card-item
            selectors = [
                ".search-card-item",
                "a[href*='/item/']",
                "div[class*='product'] a",
                "div[class*='item'] a",
            ]

            links = []
            for selector in selectors:
                try:
                    elements = await page.locator(selector).all()
                    if elements and len(elements) > 5:
                        links = elements
                        context.log.info(f"   ‚úÖ {len(links)} r√©sultats avec: {selector}")
                        break
                except:
                    continue

            if not links:
                context.log.warning("   ‚ö†Ô∏è Aucun r√©sultat trouv√©")
                return

            # Extraire les URLs
            product_urls = []
            for link in links[:self.target_results]:  # Limiter
                try:
                    href = await link.get_attribute("href")
                    if href and '/item/' in href:
                        if not href.startswith('http'):
                            href = urljoin(page.url, href)
                        # Nettoyer l'URL
                        clean_url = url_noparams(href)
                        if clean_url not in product_urls:
                            product_urls.append(clean_url)
                except:
                    continue

            context.log.info(f"   üì¶ {len(product_urls)} produits uniques extraits")

            # Ajouter les requ√™tes pour chaque produit
            for product_url in product_urls:
                await context.add_requests([
                    Request.from_url(product_url, label="ITEM")
                ])

        except Exception as e:
            context.log.error(f"   ‚ùå Erreur extraction: {e}")


async def search_aliexpress_by_image(
    image_path: str,
    category: str = "",
    output_dir: str = "output",
    max_results: int = 50
) -> Tuple[List[ImageMetadata], List[ProductData]]:
    """
    Rechercher sur AliExpress par upload d'image

    Args:
        image_path: Chemin vers l'image
        category: Cat√©gorie (non utilis√© avec upload d'image)
        output_dir: R√©pertoire de sortie
        max_results: Nombre max de r√©sultats

    Returns:
        Tuple de (image_metadata_list, product_data_list)
    """
    scraper = AliExpressImageSearchScraper(output_dir)
    return await scraper.search_by_image(image_path, category, max_results)
