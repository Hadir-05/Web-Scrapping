"""
Gestionnaire de mod√®les PyTorch avec cache Streamlit
Les mod√®les sont charg√©s une seule fois et gard√©s en m√©moire
"""
import torch
import streamlit as st
from pathlib import Path
from typing import Optional, Dict, Any
import sys

# Ajouter le chemin parent pour imports
sys.path.append(str(Path(__file__).parent.parent.parent))


@st.cache_resource
def load_keyword_model(model_path: str, device: str = "cpu"):
    """
    Charge le mod√®le de recherche par mots-cl√©s
    Utilise @st.cache_resource pour garder le mod√®le en m√©moire

    Args:
        model_path: Chemin vers le fichier .pth
        device: 'cpu' ou 'cuda'

    Returns:
        Mod√®le PyTorch charg√© et en mode eval
    """
    print(f"üîÑ Chargement du mod√®le keyword depuis {model_path}...")

    if not Path(model_path).exists():
        raise FileNotFoundError(
            f"Mod√®le non trouv√©: {model_path}\n"
            f"Placez votre mod√®le .pth dans le dossier models/"
        )

    try:
        # Charger le mod√®le
        model = torch.load(model_path, map_location=device)

        # Si c'est un state_dict, vous devrez adapter selon votre architecture
        # Exemple: model = YourModelClass(); model.load_state_dict(torch.load(...))

        # Mode √©valuation
        if hasattr(model, 'eval'):
            model.eval()

        print(f"‚úÖ Mod√®le keyword charg√© avec succ√®s sur {device}")
        return model

    except Exception as e:
        raise RuntimeError(f"Erreur lors du chargement du mod√®le: {str(e)}")


@st.cache_resource
def load_image_model(model_path: str, device: str = "cpu"):
    """
    Charge le mod√®le de recherche par image
    Utilise @st.cache_resource pour garder le mod√®le en m√©moire

    Args:
        model_path: Chemin vers le fichier .pth
        device: 'cpu' ou 'cuda'

    Returns:
        Mod√®le PyTorch charg√© et en mode eval
    """
    print(f"üîÑ Chargement du mod√®le image depuis {model_path}...")

    if not Path(model_path).exists():
        raise FileNotFoundError(
            f"Mod√®le non trouv√©: {model_path}\n"
            f"Placez votre mod√®le .pth dans le dossier models/"
        )

    try:
        # Charger le mod√®le
        model = torch.load(model_path, map_location=device)

        # Mode √©valuation
        if hasattr(model, 'eval'):
            model.eval()

        print(f"‚úÖ Mod√®le image charg√© avec succ√®s sur {device}")
        return model

    except Exception as e:
        raise RuntimeError(f"Erreur lors du chargement du mod√®le: {str(e)}")


class ModelManager:
    """
    Gestionnaire centralis√© des mod√®les avec cache
    """

    def __init__(self, keyword_path: str, image_path: str, device: str = "cpu"):
        self.keyword_path = keyword_path
        self.image_path = image_path
        self.device = device
        self._keyword_model = None
        self._image_model = None

    def get_keyword_model(self):
        """R√©cup√®re le mod√®le keyword (avec cache)"""
        if self._keyword_model is None:
            self._keyword_model = load_keyword_model(self.keyword_path, self.device)
        return self._keyword_model

    def get_image_model(self):
        """R√©cup√®re le mod√®le image (avec cache)"""
        if self._image_model is None:
            self._image_model = load_image_model(self.image_path, self.device)
        return self._image_model

    @st.cache_data
    def search_by_keyword(_self, query: str, top_k: int = 10) -> list:
        """
        Recherche par mots-cl√©s

        Args:
            query: Texte de recherche
            top_k: Nombre de r√©sultats

        Returns:
            Liste de r√©sultats [{product_id, name, score, image_url}, ...]
        """
        # PLACEHOLDER: Remplacez par votre logique de recherche
        print(f"üîç Recherche keyword: '{query}'")

        # Exemple de r√©sultats simul√©s
        # Dans votre impl√©mentation r√©elle, utilisez le mod√®le:
        # model = _self.get_keyword_model()
        # embeddings = model.encode(query)
        # results = search_in_database(embeddings)

        results = [
            {
                "product_id": f"LUX-{i+1:03d}",
                "name": f"Produit de Luxe {i+1}",
                "description": f"Description pour '{query}'",
                "score": 0.95 - (i * 0.05),
                "price": 1000 + (i * 100),
                "image_url": f"https://via.placeholder.com/300x300?text=Product+{i+1}"
            }
            for i in range(top_k)
        ]

        return results

    @st.cache_data
    def search_by_image(_self, image_data: bytes, top_k: int = 10) -> list:
        """
        Recherche par similarit√© d'image

        Args:
            image_data: Donn√©es de l'image
            top_k: Nombre de r√©sultats

        Returns:
            Liste de r√©sultats [{product_id, name, similarity, image_url}, ...]
        """
        # PLACEHOLDER: Remplacez par votre logique de recherche
        print(f"üñºÔ∏è Recherche image similarity...")

        # Exemple de r√©sultats simul√©s
        # Dans votre impl√©mentation r√©elle:
        # model = _self.get_image_model()
        # features = model.extract_features(image_data)
        # results = find_similar_images(features)

        results = [
            {
                "product_id": f"LUX-IMG-{i+1:03d}",
                "name": f"Produit Similaire {i+1}",
                "description": "Produit similaire √† votre image",
                "similarity": 0.98 - (i * 0.05),
                "price": 1500 + (i * 150),
                "image_url": f"https://via.placeholder.com/300x300?text=Similar+{i+1}"
            }
            for i in range(top_k)
        ]

        return results
